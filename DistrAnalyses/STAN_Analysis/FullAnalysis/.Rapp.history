cor.test(subset(des,Age == "Five")$CELF.Score,subset(des,Age == "Five")$u)
cor.test(subset(des,Age == "Five")$CELF.Score,subset(des,Age == "Five")$u_t)
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$u_t)
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$u)
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$u_s)
?tapply
tapply(tt$rt,tt$Subject,mean)
subj_params <- data.frame(Subject = unique(as.factor(tt$Subject)), u = colMeans(eg_stan_list$u), #
			u_s = colMeans(eg_stan_list$u_s), u_t = colMeans(eg_stan_list$u_t), mean = tapply(tt$rt,tt$Subject,mean))#
#
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$mean)
cor.test(des$CELF.Score,des$mean)
des$u_t_inv <- 1/des$u_t
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$u_t_inv)
cor.test(subset(des,Age == "Five")$CELF.Score,subset(des,Age == "Five")$u_t_inv)
cor.test(subset(des,Age == "Five")$CELF.Score,subset(des,Age == "Five")$u_t)
cor.test(subset(des,Age == "Three")$CELF.Score,subset(des,Age == "Three")$u_t)
cor.test(des$CELF.Score,des$u_t)
cor.test(des$CELF.Score,des$u_t_inv)
ggplot(des,aes(x=u,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")
ggplot(des,aes(x=u_t,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")
cor.test(des$CELF.Score,des$age)
cor.test(des$CELF.Score,des$Age)
summary(des)
cor.test(des$CELF.Score,des$Age..months.)
cor.test(des$u_t,des$Age..months.)
colMeans(eg_stan_list$beta_t)
summary(eg_stan_list$beta_t)
des$beta_t0 <- colMeans(eg_stan_list$beta_t0)
des$beta_t0 <- means(eg_stan_list$beta_t0)
des$beta_t0 <- mean(eg_stan_list$beta_t0)
summary(des)
eg_stan_list[,14:24]
eg_stan_list[,14:24] <- colMeans(eg_stan_list$beta_t)
eg_stan_list[,10:12]
eg_stan_list[10:12,]
des[,10:12]
des[,14:24] <- colMeans(eg_stan_list$beta_t)
summary(des)
names(des[,14:24])
names(des[,14:24]) <- paste("beta_t",1:10)
names(des[,14:24])
paste("beta_t",1:10)
names(des[,14:24], sep = "")
paste("beta_t",1:10, sep = "")
colnames(des[,14:24]) <- paste("beta_t",1:10, sep  = "")
names(des[,14:24])
summary(des)
colnames(des)[14:24] <- paste("beta_t",1:10, sep  = "")
colnames(des)[14:24] <- paste("beta_t",1:11, sep  = "")
summary(des)
#Â Need to read in output of stan fit first #
eg_stan_list <- extract(eg_stan_exp)#
subj_params <- data.frame(Subject = unique(as.factor(tt$Subject)), u = colMeans(eg_stan_list$u), #
			u_s = colMeans(eg_stan_list$u_s), u_t = colMeans(eg_stan_list$u_t), mean = tapply(tt$rt,tt$Subject,mean)#
			)#
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,14:24] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[14:24] <- paste("beta_t",1:11, sep  = "")
summary(des)
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,13:23] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[13:23] <- paste("beta_t",1:11, sep  = "")
summary(des)
subj_params <- data.frame(Subject = unique(as.factor(tt$Subject)), u = colMeans(eg_stan_list$u), #
			u_s = colMeans(eg_stan_list$u_s), u_t = colMeans(eg_stan_list$u_t), mean = tapply(tt$rt,tt$Subject,mean)#
			)#
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,13:23] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[13:23] <- paste("beta_t",1:11, sep  = "")#
#
des$full_t <- des$beta_t0 + u_t#
des[des$Age == "Five",]$full_t <- des[des$Age == "Five",]$full_t + des[des$Age == "Five",]$beta_t3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_t + des[des$Age == "Three",]$beta_t4
subj_params <- data.frame(Subject = unique(as.factor(tt$Subject)), u = colMeans(eg_stan_list$u), #
			u_s = colMeans(eg_stan_list$u_s), u_t = colMeans(eg_stan_list$u_t), mean = tapply(tt$rt,tt$Subject,mean)#
			)#
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,13:23] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[13:23] <- paste("beta_t",1:11, sep  = "")#
#
des$full_t <- des$beta_t0 + dex$u_t#
des[des$Age == "Five",]$full_t <- des[des$Age == "Five",]$full_t + des[des$Age == "Five",]$beta_t3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_t + des[des$Age == "Three",]$beta_t4
subj_params <- data.frame(Subject = unique(as.factor(tt$Subject)), u = colMeans(eg_stan_list$u), #
			u_s = colMeans(eg_stan_list$u_s), u_t = colMeans(eg_stan_list$u_t), mean = tapply(tt$rt,tt$Subject,mean)#
			)#
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,13:23] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[13:23] <- paste("beta_t",1:11, sep  = "")#
#
des$full_t <- des$beta_t0 + des$u_t#
des[des$Age == "Five",]$full_t <- des[des$Age == "Five",]$full_t + des[des$Age == "Five",]$beta_t3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_t + des[des$Age == "Three",]$beta_t4
summary(Des)
summary(des)
des$inv_full_t <- 1/des$full_t
summary(des)
des$inv_full_t <- 1/exp(des$full_t)
summary(des)
cor.test(des$CELF.Score,des$inv_full_t)
cor.test(des$CELF.Score,des$full_t)
cor.test(des[des$Age == "Five",]$CELF.Score,des[des$Age == "Five",]$full_t)
cor.test(des[des$Age == "Five",]$CELF.Score,des[des$Age == "Five",]$inv_full_t)
cor.test(des[des$Age == "Three",]$CELF.Score,des[des$Age == "Three",]$inv_full_t)
cor.test(des[des$Age == "Three",]$CELF.Score,des[des$Age == "Three",]$u_t)
cor.test(des[des$Age == "Five",]$CELF.Score,des[des$Age == "Five",]$u_t)
summary(des)
cor.test(des$CELF.Score,des$inv_full_t)
cor.test(des$Age..months.,des$inv_full_t)
hist(inv_full_t)
hist(des$inv_full_t)
cor.test(des[des$Age == "Three",]$CELF.Score,exp(des[des$Age == "Three",]$full_t))
cor.test(des[des$Age == "Three",]$CELF.Score,des[des$Age == "Three",]$full_t)
cor.test(des[des$Age == "Three",]$CELF.Score,des[des$Age == "Three",]$inv_full_t)
cor.test(des[des$Age == "Five",]$CELF.Score,des[des$Age == "Five",]$inv_full_t)
des$beta0 <- mean(eg_stan_list$beta0)#
des[,24:34] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[24:34] <- paste("beta",1:11, sep  = "")
summary(des)
des$full_mu <- des$beta0 + des$u#
des[des$Age == "Five",]$full_mu <- des[des$Age == "Five",]$full_mu + des[des$Age == "Five",]$beta3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_mu + des[des$Age == "Three",]$beta4
summary(eg_stan_list)
mean(eg_stan_list$beta0)
des$beta0 <- mean(eg_stan_list$beta0)#
des[,25:35] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[25:35] <- paste("beta",1:11, sep  = "")
des$full_t <- des$beta_t0 + des$u_t#
des[des$Age == "Five",]$full_t <- des[des$Age == "Five",]$full_t + des[des$Age == "Five",]$beta_t3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_t + des[des$Age == "Three",]$beta_t4#
des$inv_full_t <- 1/exp(des$full_t)#
#
des$full_mu <- des$beta0 + des$u#
des[des$Age == "Five",]$full_mu <- des[des$Age == "Five",]$full_mu + des[des$Age == "Five",]$beta3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_mu + des[des$Age == "Three",]$beta4
summary(Des)
summary(des)
descrip <- read.csv("descriptives.csv", header = T)#
des <- descrip[order(descrip$Subject),]#
#
des <- merge(des,subj_params, by = "Subject")#
#
des$beta_t0 <- mean(eg_stan_list$beta_t0)#
des[,13:23] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[13:23] <- paste("beta_t",1:11, sep  = "")#
#
des$beta0 <- mean(eg_stan_list$beta0)#
des[,25:35] <- colMeans(eg_stan_list$beta_t)#
colnames(des)[25:35] <- paste("beta",1:11, sep  = "")#
des$full_t <- des$beta_t0 + des$u_t#
des[des$Age == "Five",]$full_t <- des[des$Age == "Five",]$full_t + des[des$Age == "Five",]$beta_t3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_t + des[des$Age == "Three",]$beta_t4#
des$inv_full_t <- 1/exp(des$full_t)#
#
des$full_mu <- des$beta0 + des$u#
des[des$Age == "Five",]$full_mu <- des[des$Age == "Five",]$full_mu + des[des$Age == "Five",]$beta3#
des[des$Age == "Three",]$full_t <- des[des$Age == "Three",]$full_mu + des[des$Age == "Three",]$beta4
summary(des)
cor.test(des[des$Age == "Five",]$CELF.Score,des[des$Age == "Five",]$full_mu)
cor.test(des[des$Age == "Three",]$CELF.Score,des[des$Age == "Three",]$full_mu)
cor.test(des$CELF.Score,des$full_mu)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1a")#
print(catch)#
comp <- Comp_Import("./Exp1a")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
# RT Analyses#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" ), rt, wid = .(Subj), within = .(Stim, Task))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task == "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Task != "List"), rt, wid = .(Subj), within = .(Stim))$ANOVA#
# Acc Analyses#
ezANOVA(comp, Acc, wid = .(Subj), within = .(Stim,Task))$ANOVA#
# GLMER Analysis, Task removed for convergence#
summary(glmer(Acc ~ Task * Stim + (1+Stim|Subj) , data = comp, family = "binomial"))#
#
# Prepare variables for bar graph#
comp$DetailedTask <- "List (Cup,Tree)"#
comp[comp$Task == "Phrase",]$DetailedTask <- "Phrase (Pink Tree)"#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)", "List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("Two Words", "One Word"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
print(comp.rt)#
#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
print(comp.Acc)#
#
par(fig = c(0,1,0.35,1),mar = c(3,4,2,2))#
Comp_Graph(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Comp_Graph(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.5,1),"Accuracy")#
#
# Prepare variables for bar graph#
comp$DetailedTask <- ordered(comp$DetailedTask, levels = c("Phrase (Pink Tree)","List (Cup,Tree)"))#
comp$Stim <- ordered(comp$Stim, levels = c("One Word", "Two Words"))#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  + Subj, , data = subset(comp, Acc ==1 & Match == "Match"), FUN = c(mean), na.rm = T , keep.names = T)#
# Print RT means and c.i.s#
ci.m <- aggregate(rt ~  Stim + DetailedTask , comp.rt, mean); ci.m#
ci.l <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.low); ci.l#
ci.h <- aggregate(rt ~  Stim + DetailedTask , comp.rt, ci.high); ci.h#
#
comp.rt <- summaryBy(rt + rtAdj ~ DetailedTask + Stim  , data = comp.rt, FUN = c(mean,sd), na.rm = T )#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim  +Subj, , data = comp, FUN = c(mean), na.rm = T , keep.names = T)#
comp.Acc <- summaryBy(Acc + AccAdj~  DetailedTask + Stim   , data = comp, FUN = c(mean,sd), na.rm = T )#
#
# Function for plotting data as a ling graph#
Comp_Graph_l = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  theme_set(theme_bw())#
  DV.se <- DV.se/(sqrt(length(unique(Subj))))#
  #comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  #comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  graph_data <- data.frame(Stim = IV1, Task = IV2, DV = DV.mean, SE = DV.se)#
  print((graph_data))#
  if (leg == TRUE){#
    #x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.8), tck = -0.01)#
    # PRetty ggplot2 code drawn from https://github.com/langcog/KTE/blob/master/full%20analysis%20all%20experiments.R#
    #		#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+ #
      guides(colour=guide_legend()) +#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            legend.text = element_text(size=12),#
            legend.key = element_blank(),#
            legend.title=element_blank(),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.title.x=element_blank(),#
            legend.position=c(0.3,0.8))#
  } else{#
    x <- ggplot(graph_data, aes(x=Stim, y=DV,group = Task, linetype = Task)) + #
      ylim(ylimit) +#
      ylab(ylab) +#
      geom_line(aes(group = Task, linetype = Task),position=position_dodge(width=.1),stat="identity") + #
      geom_linerange(aes(ymin=DV - SE, ymax=DV + SE), position=position_dodge(width=.1))+#
      theme(strip.background = element_rect(fill="#FFFFFF"), #
            strip.text = element_text(size=12), #
            axis.text = element_text(size=12),#
            axis.title = element_text(size=14),#
            title = element_text(size=16),#
            panel.grid = element_blank(),#
            axis.text.x=element_blank(),#
            axis.title.x=element_blank(),#
            legend.position= "none")}#
  #arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
  return(x)#
}#
RT <- Comp_Graph_l(comp.rt$rt.mean,comp.rt$rtAdj.sd, comp.rt$Stim, comp.rt$DetailedTask, comp$Subj, paste("Two Words", "Reaction Time", sep = " "), c(700,1000),"Reaction Time (ms)",leg = TRUE)#
par(fig = c(0,1,0,0.35),mar = c(3,4,2,2), new = TRUE)#
Acc <- Comp_Graph_l(comp.Acc$Acc.mean,comp.Acc$AccAdj.sd, comp.Acc$Stim, comp.Acc$DetailedTask, comp$Subj, paste("Two Words", "Accuracy", sep = " "), c(0.7,1.03),"Accuracy")#
#
# Get the gtables#
gRT <- ggplotGrob(RT)#
gAcc <- ggplotGrob(Acc)#
#
# Set the widths#
gAcc$widths <- gRT$widths#
#
# Arrange the two charts.#
# The legend boxes are centered#
grid.newpage()#
grid.arrange(gAcc,gRT, nrow = 2, heights = c(1,2))
summary(comp.rt)
summary(comp)
summary(comp$Item)
strsplit(comp$Item[1:3])
strsplit(comp$Item[1:3],split = "_")
?strplit
?strsplit
strsplit(comp$Item[1],split = "_")
strsplit(comp$Item[1],"_")
strsplit(comp$Item[1],"_")
strsplit(comp$Item[1],[_])
strsplit(comp$Item[1],"[_]")
strsplit(as.character(comp$Item[1]),"[_]")
comp$Item[1]
strsplit(as.character(comp$Item[1]),"[_]")[2]
strsplit(as.character(comp$Item[1]),"[_]")[[1]][2]
strsplit(as.character(comp$Item),"[_]")[[1]][2][1:10]
strsplit(as.character(comp$Item),"[_]")[[]][2]
strsplit(as.character(comp$Item),"[_]") -> a
summary(a)
unique(comp$Item)
library(reshape2)
colsplit(comp$Item[1:10])
colsplit(comp$Item[1:10],"_")
colsplit(comp$Item[1:10],"_", names = c("1","2"))
colsplit(comp$Item[1:10],"_", names = c("1","2"))[2]
summary(comp)
comp$base_item <- colsplit(comp$Item,"_",names = c("word1","word"2))[2]
comp$base_item <- colsplit(comp$Item,"_",names = c("word1","word2"))[2]
summary(comp)
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[2])
summary(comp)
comp$base_item <- colsplit(comp$Item,"_",names = c("word1","word2"))[2]
comp$base_item <- as.factor(base_item)
comp$base_item <- as.factor(base_item.word2)
summary(comp)
comp$base_item <- as.factor(comp$base_item.word2)
summary(comp)
comp$base_item.word2[1:10]
comp$base_item.word2
colsplit(comp$Item,"_",names = c("word1","word2"))
colsplit(comp$Item,"_",names = c("word1","word2"))[,2]
comp$base_item <- colsplit(comp$Item,"_",names = c("word1","word2"))[,2]
summary(comp)
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])
summary(comp)
summary(lmer(rt ~ Stim * Task + (1+Stim*Task|Subj) + (1+Stim*Task|base_item), data = comp))
contrasts(comp$Stim)
contrasts(comp$Task)
contrasts(comp$Stim)[1]
contrasts(comp$Stim)[1] <- -1
contrasts(comp$Stim)[2] <- 1
contrasts(comp$Task)[1] <- -1
contrasts(comp$Task)[2] <- 1
summary(lmer(rt ~ Stim * Task + (1+Stim*Task|Subj) + (1+Stim*Task|base_item), data = comp))
summary(lmer(rt ~ Stim * Task + (1+Stim*Task||Subj) + (1+Stim*Task||base_item), data = comp))
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
contrasts(comp$Stim)[1] <- 0
contrasts(comp$Task)[1] <- 0
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
contrasts(comp$Stim)[1] <- -1
contrasts(comp$Task)[1] <- -1
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1a")#
print(catch)#
comp <- Comp_Import("./Exp1a")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Task)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Task)[2] <- 1#
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data[4]$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Disjunction"#
	#print(summary(d))#
	d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "One Word"#
	d[d$Cond %in% c("Match-Adj", "Match-Noun","Match-Color","Mismatch-Disjunc","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
	d$Match <- "Match"#
	d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun","Mismatch-Disjunc"),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.2), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
contrasts(comp$Task) <- c(-0.5,0.5)#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Task)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Task)[2] <- 1
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim+Task|base_item) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim+Task|Subj)+(1|Subj) + (0+Stim+Task|base_item) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = comp))
contrasts(comp$Stim)
contrasts(comp$Task)
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim+Task|Subj)+(1|Subj) + (0+Stim|base_item) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim+Task|Subj)+(1|Subj) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (1|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (1+Stim*Task|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (1+Stim+Task|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (1+Stim|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (1+Task|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Task|Subj)+(1|Subj) + (1+Task|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Task|Subj)+(1|Subj) + (1+Task+Stim|base_item), data = comp))
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Task|Subj)+(1|Subj) + (1+Task|base_item), data = comp))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Type)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Type)[2] <- 1#
#
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (0+Stim*Type|base_item) + (1|base_item), data = comp))
summary(comp)
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("prop1","prop2","prop3","prop4"))[,1])
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])
summary(lmer(rt ~ Stim * Task + (0+Stim|Subj)+(1|Subj) + (0+Stim*Type|base_item) + (1|base_item), data = comp))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Type)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Type)[2] <- 1#
#
# RT Analyses#
summary(lmer(rt ~ Stim * Type + (0+Stim|Subj)+(1|Subj) + (0+Stim*Type|base_item) + (1|base_item), data = comp))
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Type)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Type)[2] <- 1
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Type)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Type)[2] <- 1#
#
# RT Analyses#
summary(lmer(rt ~ Stim * Type + (0+Stim|Subj)+(1|Subj) + (0+Stim*Type|base_item) + (1|base_item), data = comp))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
#Reaction Times
?C
summary(C(comp.omni$Stim))
contrasts(C(comp.omni$Stim))
contrasts(comp.omni$Stim)
contrasts(C(comp.omni$Stim, "contr.treatment"))
contrasts(C(comp.omni$Stim, "contr.sum"))
contrasts( comp.omni$Stim) <-C(comp.omni$Stim, "contr.sum")
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))
contrasts( comp.omni$Match) <-contrasts(C(comp.omni$Match, "contr.sum"))
contrasts( comp.omni$Match)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
library(ez)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
    d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
    output <- cbind(key = d$key_press,Subj = d$Subj)#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
Comp_Import = function(path_name){#
  library(jsonlite)#
  list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
  comp = c()#
  for (x in file_list){#
    file_name = x#
    df <- fromJSON(file_name)#
    d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
    d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj"),]#
    #d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
    d$Cond <- as.factor(d$stims$Cond)#
    d$Item <- as.factor(d$stims$Item)#
    d$Task <- "List"#
    d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
    d$Task <- as.factor(d$Task)#
    d$Stim <- "One Word"#
    d[d$Cond %in% c("Match-Adj", "Match-List","Mismatch-List","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
    d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
    d$Match <- "Match"#
    d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun"),]$Match <- "MisMatch"#
    d$Match <- as.factor(d$Match)#
    output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
    output$rt <- as.numeric(as.character(output$rt))#
    comp = rbind(comp,output)#
    print(x)#
  }#
  return(comp)#
}#
#
# Function for plotting data using bar plots#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
  DV.se <- DV.se/(sqrt( (length(unique(Subj))) ))#
  comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
  comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
  if (leg == TRUE){#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
  } else{#
    x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nCup Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
    axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
  }#
  arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1a")#
print(catch)#
comp <- Comp_Import("./Exp1a")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
contrasts(comp$Task) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
#
# Calcs for within subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
  comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
  comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
}#
#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Task)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Task)[2] <- 1#
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim*Task|base_item) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (0+Stim*Task|Subj)+(1|Subj) + (0+Stim+Task|base_item) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (0+Stim+Task|Subj)+(1|Subj) + (0+Stim+Task|base_item) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (0+Stim+Task|Subj)+(1|Subj) + (0+Stim|base_item) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (0+Task|Subj)+(1|Subj) + (0+Stim|base_item) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1|Subj) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task|Subj) + (1|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task|Subj) + (1+Task|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task+Stim|Subj) + (1+Task|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task+Stim|Subj) + (1+Task+Stim|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task*Stim|Subj) + (1+Task+Stim|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
summary(lmer(rt ~ Stim * Task + (1+Task*Stim|Subj) + (1+Task*Stim|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data[4]$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Disjunction"#
	#print(summary(d))#
	d[d$Cond %in% c("Match-Adj", "Match-Mask-Adj","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ),]$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "One Word"#
	d[d$Cond %in% c("Match-Adj", "Match-Noun","Match-Color","Mismatch-Disjunc","Mismatch-Color", "Mismatch-Noun"),]$Stim <- "Two Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words"))#
	d$Match <- "Match"#
	d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-Disjunc","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun","Mismatch-Disjunc"),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Match = d$Match)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 2.2), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Composition Task\nPink Tree","List Task\nPink, Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
catch <- Catch_Import("./Exp1b")#
print(catch)#
comp <- Comp_Import("./Exp1b")#
contrasts(comp$Stim) <- c(-0.5,0.5)#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
contrasts(comp$Task) <- c(-0.5,0.5)#
# Prep for w-subj SEs#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Task)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Task)[2] <- 1#
#
# RT Analyses#
summary(lmer(rt ~ Stim * Task + (1+Task*Stim|Subj) + (1+Task*Stim|base_item), data = subset(comp, Acc ==1 & Match == "Match" )))
library(jsonlite)#
library(ggplot2)#
library(gridExtra)#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d$Stim <- ordered(d$Stim, levels = c("Two Words", "One Word"))#
	d$PicType <- ifelse(d$Type == "Color", "Variable Colors (as Experiment 1)","Fixed Colors")#
	d$PicType <- ordered(d$PicType, levels = c("Variable Colors (as Experiment 1)", "Fixed Colors"))#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block <- rep(c(1,2), each = 100)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Length = d$Length, Block = d$Block)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt( (length(unique(Subj))/2) ))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),  col = c("gray47"), density = c(40,100), args.legend = list(bty = "n", x = 5), tck = -0.01)#
} else{#
x <- barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Mismatched Predictability\nPink Tree","Matched Predictability\nPink Tree"),   col = c("gray47"), density = c(40,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(x, (c(comp.graph.mean) + c(comp.graph.se)+0.01), x, (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp1c")#
print(catch)#
comp <- Comp_Import("./Exp1c")#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])#
contrasts(comp$Stim)[1] <- -1#
contrasts(comp$Type)[1] <- -1#
contrasts(comp$Stim)[2] <- 1#
contrasts(comp$Type)[2] <- 1#
#
# RT Analyses#
summary(lmer(rt ~ Stim * Type + (1+Stim|Subj)+ (1+Stim*Type|base_item) , data = subset(comp, Acc ==1 & Match == "Match")))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
#Reaction Times
contrasts( comp.omni$Match)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2"))[,2])#
#
contrasts( comp.omni$Match) <-contrasts(C(comp.omni$Match, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp$base_item <- as.factor(colsplit(comp$Item,"_",names = c("word1","word2","word3"))[,3])#
#
contrasts( comp.omni$Match) <-contrasts(C(comp.omni$Match, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))
unique(comp$Item)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp.omni$base_item <- as.factor(colsplit(comp.omni$Item,"_",names = c("word1","word2","word3"))[,3])#
#
contrasts( comp.omni$Match) <-contrasts(C(comp.omni$Match, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))
summary(comp.omni)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp.omni$base_item <- as.factor(colsplit(comp.omni$Item,"_",names = c("word1","word2","word3"))[,3])#
#
contrasts( comp.omni$Type) <-contrasts(C(comp.omni$Type, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))#
#
#Reaction Times#
summary(lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase"))
)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp.omni$base_item <- as.factor(colsplit(comp.omni$Item,"_",names = c("word1","word2","word3"))[,3])#
#
contrasts( comp.omni$Type) <-contrasts(C(comp.omni$Type, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))#
#
#Reaction Times#
summary(lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")))
summary(lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")))
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")) -> a
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")) -> a.1
anova(a,a.1)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(ez)#
library(lme4)#
library(doBy)#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Comp_Import_light= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("Three Words", "Two Words", "One Word"))#
	d$PicType <- "Dark"#
	d[grep("light_",d$Item, ignore.case = TRUE),]$PicType <- "Light"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import_Big = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	#d <- d[d$stims$Cond %in% c("Mismatch-Mask-List","Mismatch-List","Mismatch-Mask-Adj", "Mismatch-Color", "Mismatch-Noun" ,"Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj", "Mismatch-Disjunc", "Match-Noun", "Match-Color"),]#
	#d <- d[d$stims$Cond %in% c("Match-Mask-List","Match-List","Match-Mask-Adj", "Match-Adj" ),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Item <- as.factor(d$stims$Item)#
	d$Task <- "Phrase"#
	if (length(d[grep("list",d$Cond, ignore.case = TRUE),]$Task) > 0){#
		d[grep("list",d$Cond, ignore.case = TRUE),]$Task <- "List"#
		}#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[grep("full-mask",d$Cond, ignore.case = TRUE),]$Stim <- "One Word"#
	d[grep("no-mask",d$Cond, ignore.case = TRUE),]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Big"#
	d[grep("small_",d$Item, ignore.case = TRUE),]$PicType <- "Small"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Item = d$Item, Cond = d$Cond, Task = d$Task, Stim = d$Stim, Pic = d$stimulus, Match = d$Match, PicType = d$PicType)#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
comp_process = function(comp){#
		#contrasts(comp$Stim) <- c(-0.5,0.5)#
		#contrasts(comp$Task) <- c(-0.5,0.5)#
		comp <- comp[comp$rt > 300 & comp$rt <1500,]#
		comp$Acc <- 0#
		comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
		comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
		comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
		comp$rtAdj <- NA#
		comp$AccAdj <- NA#
		for (i in unique(comp$Subj)){#
			comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
			comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
			}#
	return(comp)#
	}#
# Function for plotting data#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.2), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Unstructured Composition\nBig Pink Tree","Structured Composition\nDark Pink Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
comp.1 <- comp_process(Comp_Import_Big("./Exp2/Exp2Big/data"))#
comp.1$Type <- "Big"#
comp.2 <- comp_process(Comp_Import_light("./Exp2/Exp2Dark/data"))#
comp.2$Type <- "Dark"#
#
comp.omni <- rbind(comp.1,comp.2)#
comp.omni$PicType = NA#
for (colors in c("big", "small","dark","light")){#
	comp.omni[grep(colors,comp.omni$Pic, ignore.case = TRUE),]$PicType <- colors#
	}#
comp.omni$PicType <- as.factor(comp.omni$PicType)#
#
comp.omni$Type <- as.factor(comp.omni$Type)#
summary(comp.omni)#
#
comp.omni$base_item <- as.factor(colsplit(comp.omni$Item,"_",names = c("word1","word2","word3"))[,3])#
#
contrasts( comp.omni$Type) <-contrasts(C(comp.omni$Type, "contr.sum"))#
contrasts( comp.omni$Stim) <-contrasts(C(comp.omni$Stim, "contr.sum"))#
#
#Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")) -> omni.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")) -> omni.noint#
anova(omni.full,omni.noint)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d[d$Length == "3",]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Striped"#
	d[grep("spotted",d$stims$Pic, ignore.case = TRUE),]$PicType <- "Spotted"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block = rep(c(1,2), each = 150)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Block = d$Block, File = d$stimulus)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.5), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp3")#
print(catch)#
comp <- Comp_Import("./Exp3")#
comp <- subset(comp, Type %in% c("Adj3","Adj4", "Adv4"))#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}
summary(comp)
contrasts( comp$Type) <-contrasts(C(comp$Type, "contr.sum"))#
contrasts( comp$Stim) <-contrasts(C(comp$Stim, "contr.sum"))
summary(lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase")))
summary(lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp.omni, Acc ==1 & Match == "Match" & Task == "Phrase" & Type %in% c("Adj3","Adj4", "Adv4"))))
summary(lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Task == "Phrase" & Type %in% c("Adj3","Adj4", "Adv4"))))
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.noint#
anova(exp3.full,exp3.noint)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.noint#
anova(exp3.full,exp3.noint)
1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "One Word"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA#
ezANOVA(subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4") & Stim != "Three Words"), rt, wid = .(Subj), within = .(Stim), between = .(Type))$ANOVA
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.noint#
anova(exp3.full,exp3.noint)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim*Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.noint#
anova(exp3.full,exp3.noint)
library(ggplot2)#
library(gridExtra)#
library(grid)#
library(jsonlite)#
#
# This script is used to read in all the csv files in a folder.#
#
library(doBy)#
## for bootstrapping 95% confidence intervals -- from Mike Frank https://github.com/langcog/KTE/blob/master/mcf.useful.R#
library(bootstrap)#
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}#
ci.low <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)} #  mean(x,na.rm=na.rm) -#
ci.high <- function(x,na.rm=T) {#
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) } #- mean(x,na.rm=na.rm)}#
Catch_Import= function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[1:2,]   ##df$data[4]$trialdata$key_press %in% c(71,32),]#
	d$Subj <- unique(df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]$Subj)[2]#
	output <- cbind(key = d$key_press,Subj = d$Subj)#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
Comp_Import = function(path_name){#
library(jsonlite)#
#
list.files(path = path_name,full.names = T, pattern = ".txt") -> file_list#
comp = c()#
for (x in file_list){#
	file_name = x#
	df <- fromJSON(file_name)#
	d <- df$data$trialdata[df$data$trialdata$Screen == "Real-Response",]#
	d <- d[ grep("match",d$stims$Cond, ignore.case = TRUE),]#
	d$Cond <- as.factor(d$stims$Cond)#
	d$Type <- as.factor(d$stims$Type)#
	d$Phrase <- as.factor(d$stims$Phrase)#
	d$Length <- as.factor(d$stims$Length)#
	d$Task <- "Phrase"#
	d$Task <- as.factor(d$Task)#
	d$Stim <- "Two Words"#
	d[d$Length == "1",]$Stim <- "One Word"#
	d[d$Length == "3",]$Stim <- "Three Words"#
	d$Stim <- ordered(d$Stim, levels = c("One Word", "Two Words", "Three Words"))#
	d$PicType <- "Striped"#
	d[grep("spotted",d$stims$Pic, ignore.case = TRUE),]$PicType <- "Spotted"#
	d$Match <- "Match"#
	d[grep("mismatch",d$Cond, ignore.case = TRUE),]$Match <- "MisMatch"#
	d$Block = rep(c(1,2), each = 150)#
	d$Match <- as.factor(d$Match)#
	output <- data.frame(rt = as.numeric(as.character(d$rt)), key_press = d$key_press, Subj = d$Subj, Cond = d$Cond, Type = d$Type, Task = d$Task, Stim = d$Stim, Pic = d$stims$Pic, Phrase = d$Phrase, Match = d$Match, PicType = d$PicType, Block = d$Block, File = d$stimulus)#
	#print(summary(d))#
	output$rt <- as.numeric(as.character(output$rt))#
	comp = rbind(comp,output)#
	print(x)#
	}#
	return(comp)#
}#
#
# Function for plotting data#
Comp_Graph = function(DV.mean, DV.se, IV1, IV2, Subj, title,ylimit,ylab,leg = FALSE){#
DV.se <- DV.se/(sqrt(length(unique(Subj))))#
comp.graph.mean <- tapply(DV.mean,list(IV1, IV2), mean)#
comp.graph.se <- tapply(DV.se,list(IV1, IV2), mean)#
if (leg == TRUE){#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab, legend = T, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),  col = c("gray47"), density = c(40,65,100), args.legend = list(bty = "n", x = 3.5), tck = -0.01)#
} else{#
barplot(comp.graph.mean, beside = T, ylim = ylimit, ylab = ylab,  legend = F, xpd = FALSE, names.arg = c("Simple Structure\nBig Striped Tree","Complex Structure\nBig Striped Tree"),   col = c("gray47"), density = c(40,65,100), tick = FALSE, axes = FALSE)#
axis(2, at = c(0.5,0.75,1), labels = c(0.5,0.75,1.0), tck = -0.03)#
}#
arrows(c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) + c(comp.graph.se)+0.01), c(1.5,2.5,3.5,5.5,6.5,7.5), (c(comp.graph.mean) - c(comp.graph.se)-0.01), code = 0)#
}#
library(lme4)#
library(ez)#
catch <- Catch_Import("./Exp3")#
print(catch)#
comp <- Comp_Import("./Exp3")#
comp <- subset(comp, Type %in% c("Adj3","Adj4", "Adv4"))#
comp$Acc <- 0#
comp[comp$key_press == 77 & comp$Match == "Match",]$Acc <- 1#
comp[comp$key_press == 90 & comp$Match == "MisMatch",]$Acc <- 1#
comp$Task <- factor(comp$Task, levels(comp$Task)[c(2,1)])#
comp <- comp[comp$rt > 300 & comp$rt <1500,]#
comp$rtAdj <- NA#
comp$AccAdj <- NA#
for (i in unique(comp$Subj)){#
	comp[comp$Subj == i,]$rtAdj <- ((comp[comp$Subj == i,]$rt - mean(comp[comp$Subj == i,]$rt, na.rm = T)) + mean(comp$rt, na.rm = T))#
	comp[comp$Subj == i,]$AccAdj <- ((comp[comp$Subj == i,]$Acc - mean(comp[comp$Subj == i,]$Acc, na.rm = T)) + mean(comp$Acc, na.rm = T))#
	}#
comp$base_item <- as.factor(colsplit(comp$Pic,"_",names = c("prop1","prop2","prop3","prop4"))[,1])#
contrasts( comp$Type) <-contrasts(C(comp$Type, "contr.sum"))#
contrasts( comp$Stim) <-contrasts(C(comp$Stim, "contr.sum"))#
#
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.full.two-three#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.noint.two-three#
anova(exp3.full.two-three,exp3.noint.two-three)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.full.one-two#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.noint.one-two#
anova(exp3.full.one-two,exp3.noint.one-two)#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.full.two-three.adj3#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.noint.two-three.adj3#
anova(exp3.full.two-three.adj3,exp3.noint.two-three.adj3)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.full.two-three.adj4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.noint.two-three.adj4#
anova(exp3.full.two-three.adj4,exp3.noint.two-three.adj4)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.full.two-three.adv4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.noint.two-three.adv4#
anova(exp3.full.two-three.adv4,exp3.noint.two-three.adv4)
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.full.two_three#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.noint.two_three#
anova(exp3.full.two-three,exp3.noint.two-three)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.full.one_two#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.noint.one_two#
anova(exp3.full.one-two,exp3.noint.one-two)#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.full.two_three.adj3#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.noint.two_three.adj3#
anova(exp3.full.two-three.adj3,exp3.noint.two-three.adj3)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.full.two_three.adj4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.noint.two_three.adj4#
anova(exp3.full.two-three.adj4,exp3.noint.two-three.adj4)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.full.two_three.adv4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.noint.two_three.adv4#
anova(exp3.full.two-three.adv4,exp3.noint.two-three.adv4)
# Reaction Times#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.full#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4"))) -> exp3.noint#
anova(exp3.full,exp3.noint)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.full.two_three#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "One Word")) -> exp3.noint.two_three#
anova(exp3.full.two_three,exp3.noint.two_three)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.full.one_two#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3","Adj4", "Adv4")& Stim != "Three Words")) -> exp3.noint.one_two#
anova(exp3.full.one_two,exp3.noint.one_two)#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.full.two_three.adj3#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj3")& Stim != "One Word")) -> exp3.noint.two_three.adj3#
anova(exp3.full.two_three.adj3,exp3.noint.two_three.adj3)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.full.two_three.adj4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adj4")& Stim != "One Word")) -> exp3.noint.two_three.adj4#
anova(exp3.full.two_three.adj4,exp3.noint.two_three.adj4)#
#
lmer(rt ~ Stim * Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.full.two_three.adv4#
lmer(rt ~ Stim + Type  + (1+Stim|Subj) + (1+Stim+Type|base_item), data = subset(comp, Acc ==1 & Match == "Match" & Type %in% c("Adv4")& Stim != "One Word")) -> exp3.noint.two_three.adv4#
anova(exp3.full.two_three.adv4,exp3.noint.two_three.adv4)
summary(exp3.full.two_three.adv)
summary(exp3.full.two_three.adv4)
summary(exp3.full.two_three.adv4)
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 4000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 4000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 6000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 6000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 8000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 4000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 4, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
library(retimes)#
library(rstan)#
library(ggplot2)#
library(retimes)#
library(rstan)#
library(ggplot2)#
rstan_options(auto_write = TRUE)#
options(mc.cores = parallel::detectCores())#
set.seed(123)#
#
ads <- read.csv("Adult_R.csv")#
fives <- read.csv("5yo_R.csv")#
threes <- read.csv("3yo_R.csv")#
#
ads$Age <- "Adult"#
fives$Age <- "Five"#
threes$Age <- "Three"#
#
tt <- rbind(ads,fives,threes)#
tt$Age <- as.factor(tt$Age)#
tt$Subject <- paste(tt$Age,tt$Participant, sep = "")#
#
tt<- subset(tt, RTms <= 4000)#
# I should really try with a lower cutoff. 4s?#
tt$rt <- tt$RTms#
tt$N_Match <- ifelse(tt$Match == "Match",0,1)#
tt$N_Pred <- ifelse(tt$Pred == "Pred",0,1)#
tt$N_AgeFive <- model.matrix(~tt$Age)[,2]#
tt$N_AgeThree <- model.matrix(~tt$Age)[,3]#
tt$N_M_P_Interact <- tt$N_Pred * tt$N_Match#
tt$N_Match_AgeFive_Interact <-  tt$N_Match * tt$N_AgeFive#
tt$N_Match_AgeThree_Interact <- tt$N_Match * tt$N_AgeThree#
tt$N_Pred_AgeFive_Interact <- tt$N_Pred * tt$N_AgeFive#
tt$N_Pred_AgeThree_Interact <- tt$N_Pred * tt$N_AgeThree#
tt$N_Match_Pred_AgeFive_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeFive#
tt$N_Match_Pred_AgeThree_Interact <- tt$N_Match * tt$N_Pred * tt$N_AgeThree#
# tt$rt_scale <- (tt$rt - mean(tt$rt,na.rm = T))/sd(tt$rt, na.rm = T)#
#
tt$rt <- (tt$rt - mean(tt$rt))/sd(tt$rt)#
ggplot(tt,aes(x=rt,..density..,col=Age))+ geom_freqpoly(alpha=1,lwd =1.5)+xlab("Response Time (ms)")#
#
# For some reason, model won't converge with RTs above zero?#
#tt$rt <- tt$rt + abs(min(tt$rt))#
# Fit Ex-Gaussian using ML (retimes library) #
eg_ml <- timefit(tt$rt)#
print(eg_ml)#
eg_stan_exp_4s <- stan(file="fixEf_Age_and_Conds_transf_exp2.stan",#
                data=stanDat_full,#
                 chains = 2, iter = 2000, control = list(adapt_delta = 0.88))#
print(eg_stan_exp, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
print(eg_stan_exp_4s, pars = c("beta0","beta","beta_s0","beta_s","beta_t0","beta_t"), probs = c(0.025,0.5,0.975))
